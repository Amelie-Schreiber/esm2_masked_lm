{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of Highest and Lowest Fitness Proteins\n",
    "\n",
    "In this notebook we use a persistent homology informed DBSCAN to cluster the top `m` predicted protein sequences, and the bottom `m` protein sequences generated by ESM-2 for masked language modeling. In particular, we take a protein sequence, mask several positions in the protein sequence. Next, we compute a simple additive score of the fitness of a protein based on the sum of the log probabilities obtained from the logits for the masked sequence. Retrieving the top and bottom `m` predictions from the model, we then compute the persistent homology and persistence diagrams of each predicted sequence. Next, we compute the Wasserstein distances between each pair of persistence diagrams obtaining a distance matrix. We then compute persistent homology of this distance matrix, and use this to inform the parameter selection of a DBSCAN to obtain more nuanced clusters than those provided by $k$-Means of Agglomerative Clustering. Substituting HDBSCAN for DBSCAN allows us to form a hierarchical clustering that yields a tree of clusters which might be akin to a something like a phylogenetic tree. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ph_clustering_sequences as phcs\n",
    "import predict_top_sequences as pts \n",
    "\n",
    "from transformers import AutoTokenizer, EsmModel, EsmForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Protein Sequences**: \n",
    "\n",
    "   The sequences represent the primary structure of proteins which is a linear chain of amino acids. Each sequence $S$ is represented as a string of characters, where each character represents a type of amino acid. We mask several positions, and then retrieve the top and bottom `m` predicted sequences given by ESM-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "sequence = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"\n",
    "mask_positions = [67, 82, 83]\n",
    "m = 10\n",
    "scores, bottom_full_sequences = pts.predict_bottom_full_sequences_nst(sequence, mask_positions, m)\n",
    "scores, bottom_full_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top `m` sequences\n",
    "sequence = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"\n",
    "mask_positions = [67, 82, 83]\n",
    "m = 10\n",
    "scores, top_full_sequences = pts.predict_top_full_sequences(sequence, mask_positions, m)\n",
    "scores, top_full_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [top_full_sequences, bottom_full_sequences]\n",
    "flattened_sequences = [seq for sublist in sequences for seq in sublist]\n",
    "# flattened_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Persistence Diagrams**: \n",
    "\n",
    "   Topological Data Analysis (TDA) is a field that allows us to study the shape of our data. One of the tools in TDA is persistent homology, which is a method for identifying and quantifying features of a dataset such as loops, voids, and connected components. A persistence diagram is a multi-set of points in the plane that summarize the topological features of a dataset discovered during a filtration process. \n",
    "\n",
    "   Mathematically, a persistence diagram $D$ is a finite multiset of points $(b,d) \\in \\mathbb{R}^2$ where $b < d$. Each point $(b,d)$ represents a topological feature that appears at filtration value $b$ and disappears at $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define layer to be used\n",
    "num_layers = model.config.num_hidden_layers\n",
    "layer = num_layers - 1  # Index of the last layer\n",
    "sequences = flattened_sequences\n",
    "\n",
    "# Initialize list to store persistent diagrams\n",
    "persistent_diagrams = []\n",
    "\n",
    "# Compute persistent homology for each sequence\n",
    "for sequence in sequences:\n",
    "    hidden_states_matrix = phcs.get_hidden_states(tokenizer, model, layer, sequence)\n",
    "    distance_matrix = phcs.compute_euclidean_distance_matrix_scipy(hidden_states_matrix)\n",
    "    _, persistence_diagram = phcs.compute_persistent_homology(distance_matrix)\n",
    "    \n",
    "    # Store the persistent diagram\n",
    "    persistent_diagrams.append(persistence_diagram)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Wasserstein Distances**:\n",
    "\n",
    "   The Wasserstein distance, also known as the Earth Mover's distance, is a measure of the distance between two probability distributions. Given two persistence diagrams $D_1$ and $D_2$, the $p$-Wasserstein distance $W_p(D_1, D_2)$ between $D_1$ and $D_2$ is defined as:\n",
    "\n",
    "   $$\n",
    "   W_p(D_1, D_2) = \\left(\\inf_{\\gamma \\in \\Gamma(D_1, D_2)} \\sum_{(x,y) \\in \\gamma} \\|x - y\\|^p \\right)^{1/p}\n",
    "   $$\n",
    "\n",
    "   where $\\Gamma(D_1, D_2)$ denotes the set of all bijections $\\gamma: D_1 \\cup \\Delta \\to D_2 \\cup \\Delta$, and $\\Delta$ is the diagonal in $\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Wasserstein distances between all pairs of persistence diagrams\n",
    "wasserstein_distances = phcs.compute_wasserstein_distances(persistent_diagrams, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Computing persistent homology once more, we get a persistence diagram for the collection of persistence diagrams we computed. This gives a summary of the differences in the protein sequences from a topological perspective. We can use it to inform our DBSCAN later on, but it is not needed for the $k$-Means, Agglomerative Clustering, or HDBSCAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the persistent homology of the Wasserstein distance matrix\n",
    "st_2, persistence_2 = phcs.compute_persistent_homology(wasserstein_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "\n",
    "# Plot the persistence diagram\n",
    "gd.plot_persistence_diagram(persistence_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Clustering**: \n",
    "\n",
    "   After computing the Wasserstein distances, we can use a clustering algorithm to group similar sequences together. Clustering algorithms like HDBSCAN, KMeans, Agglomerative Clustering, and DBSCAN work by grouping data points that are close to each other according to a given distance metric. \n",
    "\n",
    "   In this case, the distance metric is the Wasserstein distance. Thus, two sequences will be in the same cluster if their persistence diagrams (and hence their topological features) are close to each other in terms of the Wasserstein distance. \n",
    "\n",
    "5. **Minimum Spanning Tree and Cluster Hierarchy**: \n",
    "\n",
    "   For HDBSCAN, a minimum spanning tree of the distance weighted graph is built. The tree is then used to construct a cluster hierarchy of connected components. This hierarchy is then pruned based on the stability of clusters.\n",
    "\n",
    "6. **Extracting Clusters**: \n",
    "\n",
    "   The final step is to extract the stable clusters from the cluster hierarchy to get a flat clustering. This is done by selecting clusters that have the greatest persistence, or in other words, that exist over a wide range of distance scales. This gives us a set of clusters that represent the main topological features of our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(wasserstein_distances)\n",
    "labels_kmeans = kmeans.labels_\n",
    "print(\"Silhouette Coefficient for KMeans: %0.3f\" % silhouette_score(wasserstein_distances, labels_kmeans))\n",
    "\n",
    "# Perform Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3, affinity='precomputed', linkage='average').fit(wasserstein_distances)\n",
    "labels_agg = agg_clustering.labels_\n",
    "print(\"Silhouette Coefficient for Agglomerative Clustering: %0.3f\" % silhouette_score(wasserstein_distances, labels_agg))\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "dbscan = DBSCAN(metric=\"precomputed\", eps=0.270, min_samples=2).fit(wasserstein_distances)\n",
    "labels_dbscan = dbscan.labels_\n",
    "if len(set(labels_dbscan)) > 1:  # More than 1 cluster\n",
    "    print(\"Silhouette Coefficient for DBSCAN: %0.3f\" % silhouette_score(wasserstein_distances, labels_dbscan))\n",
    "else:\n",
    "    print(\"Cannot compute Silhouette Coefficient for DBSCAN as there is only one cluster.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the clusters for KMeans\n",
    "print(\"Clusters for KMeans:\")\n",
    "kmeans_clusters = {i: [] for i in set(labels_kmeans)}\n",
    "for sequence, label in zip(sequences, labels_kmeans):\n",
    "    kmeans_clusters[label].append(sequence)\n",
    "for label, cluster in kmeans_clusters.items():\n",
    "    print(f\"Cluster {label}: {cluster}\")\n",
    "\n",
    "# Print the clusters for Agglomerative Clustering\n",
    "print(\"\\nClusters for Agglomerative Clustering:\")\n",
    "agg_clusters = {i: [] for i in set(labels_agg)}\n",
    "for sequence, label in zip(sequences, labels_agg):\n",
    "    agg_clusters[label].append(sequence)\n",
    "for label, cluster in agg_clusters.items():\n",
    "    print(f\"Cluster {label}: {cluster}\")\n",
    "\n",
    "# Print the clusters for DBSCAN\n",
    "print(\"\\nClusters for DBSCAN:\")\n",
    "dbscan_clusters = {i: [] for i in set(labels_dbscan)}\n",
    "for sequence, label in zip(sequences, labels_dbscan):\n",
    "    dbscan_clusters[label].append(sequence)\n",
    "for label, cluster in dbscan_clusters.items():\n",
    "    print(f\"Cluster {label}: {cluster}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Calculate ARIs\n",
    "ari_kmeans_agg = adjusted_rand_score(labels_kmeans, labels_agg)\n",
    "ari_kmeans_dbscan = adjusted_rand_score(labels_kmeans, labels_dbscan)\n",
    "ari_agg_dbscan = adjusted_rand_score(labels_agg, labels_dbscan)\n",
    "\n",
    "# Print ARIs\n",
    "print(f\"Adjusted Rand Index for KMeans and Agglomerative Clustering: {ari_kmeans_agg}\")\n",
    "print(f\"Adjusted Rand Index for KMeans and DBSCAN: {ari_kmeans_dbscan}\")\n",
    "print(f\"Adjusted Rand Index for Agglomerative Clustering and DBSCAN: {ari_agg_dbscan}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Create an HDBSCAN instance with gen_min_span_tree set to True\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=5, gen_min_span_tree=True)\n",
    "\n",
    "# Fit the model with the data (distance matrix)\n",
    "clusterer.fit(wasserstein_distances)\n",
    "\n",
    "# Generate the minimum spanning tree\n",
    "minimum_spanning_tree = clusterer.minimum_spanning_tree_\n",
    "\n",
    "# If the minimum spanning tree was correctly generated, it should not be None\n",
    "if minimum_spanning_tree is not None:\n",
    "    minimum_spanning_tree.plot(edge_cmap='viridis', edge_alpha=0.6, node_size=80, edge_linewidth=2)\n",
    "else:\n",
    "    print(\"The minimum spanning tree was not generated. Please check the parameters and the input data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "# Create a condensed distance matrix\n",
    "condensed_dist_matrix = squareform(wasserstein_distances)\n",
    "\n",
    "# Use HDBSCAN on the distance matrix\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='precomputed', gen_min_span_tree=True)\n",
    "clusterer.fit(wasserstein_distances)\n",
    "\n",
    "# Obtain labels assigned by the HDBSCAN algorithm\n",
    "labels_hdbscan = clusterer.labels_\n",
    "\n",
    "# Group sequences by their cluster labels\n",
    "clusters_hdbscan = {i: [] for i in set(labels_hdbscan)}\n",
    "for i, label in enumerate(labels_hdbscan):\n",
    "    clusters_hdbscan[label].append(sequences[i])\n",
    "\n",
    "clusters_hdbscan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
