{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Small Token Classifier with Three Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import EsmTokenizer, EsmForTokenClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160955ab859748418ce279e948b0a82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14.9385, 'train_samples_per_second': 14.727, 'train_steps_per_second': 0.669, 'train_loss': 1.098867130279541, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "from transformers import EsmForTokenClassification\n",
    "\n",
    "class ProteinData:\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Your dataset\n",
    "dataset = {\n",
    "    'sequence': [\n",
    "        \"AVILMFCCCYRRNDCQKSTWRNDKNDKNDKNDRRNDCQKSRRNDCQKSAVILMFYKNDKNDKN\", # Protein 1\n",
    "        \"RRNDCQKSRRNDCQKSTWAVILMFCCCYRRNDCQKSTWRNDKNDKNDKNDRRNDCQKS\", # Protein 2\n",
    "        \"AVILMFYKNDKNDKNDRRNDCQKSAVILMFYKNDKNDKNRRNDCQKSTWRRNDCQKSTW\", # Protein 3\n",
    "        \"RRNDCQKSAVILMFYKNDKNDKNDRRNDCQKSRRNDCQKSTWAVILMFCCCYRRNDCQKSTW\", # Protein 4\n",
    "        \"TWRRNDCQKSAVILMFYKNDKNDRRNDCQKSTWRRNDCQKSAVILMFYKNDKNDKN\", # Protein 5\n",
    "        \"MVLSEGEWQLVLHVWAKVEADVAGHGQDILIRLFKSHPETLEKFDRVKHLKTEAEMKASEDLKKHGVTVLTALGAILKKKGHHEAELKPLAQSHATKHKIPIKYLEFISDAIIHVLHAKHPS\",\n",
    "        \"MGSDKIHHHHHHENLYFQGADPKDLAHLLDYFEHKETDGLAKGFGTAKSVFKDATNFAEIISVLKKMRPILFLPLLCVILIFKIKFWT\",\n",
    "        \"MKLAILVTIVALVAMYRINHRTQELIELSNKNQPYTINADIEEIELTNRYPALIEYVQQQDKPLPKN\", \n",
    "        \"MPRYKILNSKLTGEKMSLYEFLVTFISKIITVLLTVFLNRYHRRWYHG\",\n",
    "        \"PMLKRRTYNLIYIAFLTVFSNKTYRIDGFIKNLPYLFRGVNNTGKPKL\",\n",
    "        \"AMNRLIFPLIKILILLLSIPFFLGNLIDKSYLKQIGLKVTFLFMLRYH\",\n",
    "        \"MFLKTLLIILWVAAIKNLQTVYQLIRFLKISRKRYEHGKNFVRIWLYK\",\n",
    "        \"VIIGDRLVRIWLYKLFIKNLIKNLPYLFLISKNLVTFLMLLRIWLYKQ\",\n",
    "        \"QIGLKVTFLMLLRGWKSVYFFLKNLPYLGFSKKNLVTFLMLLRIWLYK\",\n",
    "        \"NLKNLPYLGFSKKNLVTFLMLLRGWKSVYFFLKNLPYLFLISKNLVTFL\",\n",
    "        \"MLLRGWKSVYFFLKNLPYLFLISKNLVTFLMLLRGWKSVYFFLKNLPY\",\n",
    "        \"LFLISKNLVTFLMLLRGWKSVYFFLKNLPYLFLISKNLVTFLMLLRGW\",\n",
    "        \"KSVYFFLKNLPYLFLISKNLVTFLMLLRGWKSVYFFLKNLPYLFLISK\",\n",
    "        \"NLVTFLMLLRGWKSVYFFLKNLPYLFLISKNLVTFLMLLRGWKSVYFF\",\n",
    "        \"LKNLPYLFLISKNLVTFLMLLRGWKSVYFFLKNLPYLFLISKNLVTFLM\",\n",
    "        \"LLRGWKSVYFFLKNLPYLFLISKNLVTFLMLLRGWKSVYFFLKNLPYLF\",\n",
    "        \"LISKNLVTFLMLLRGWKSVYFFLKNLPYLFLISKNLVTFLMLLRGWKSV\"\n",
    "    ],\n",
    "    'labels': [\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], # Labels for Protein 1\n",
    "        [2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1], # Labels for Protein 2\n",
    "        [0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0], # Labels for Protein 3\n",
    "        [2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1], # Labels for Protein 4\n",
    "        [2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1],  # Labels for Protein 5\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2],\n",
    "        [0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2],\n",
    "        [2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0],\n",
    "        [0, 2, 1, 0, 1, 2, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 1, 0],\n",
    "        [1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1, 2],\n",
    "        [2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 0, 2, 1],\n",
    "        [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1, 0, 2],\n",
    "        [2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0, 1, 2, 0, 2, 1, 0],\n",
    "        [1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2],\n",
    "        [0, 1, 2, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 1, 2],\n",
    "        [2, 0, 1, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 1, 2, 0],\n",
    "        [1, 2, 0, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 2, 0, 1, 2],\n",
    "        [2, 1, 0, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1],\n",
    "        [0, 2, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 1, 0, 2],\n",
    "    ]\n",
    "}\n",
    "# 0: Exposed to Solvent\n",
    "# 1: Binding Site\n",
    "# 2: Transmembrane region\n",
    "\n",
    "# Pad the sequences and labels\n",
    "for i in range(len(dataset['sequence'])):\n",
    "    while len(dataset['sequence'][i]) < 80:\n",
    "        dataset['sequence'][i] += 'X' # padding token for sequence\n",
    "        dataset['labels'][i].append(-100) # padding token for labels\n",
    "\n",
    "# Create the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "# Create the model and specify the number of labels\n",
    "model = EsmForTokenClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=3)\n",
    "\n",
    "\n",
    "# Convert sequences to input IDs and labels to tensors\n",
    "inputs = [tokenizer(seq, truncation=True, padding='max_length', max_length=50)[\"input_ids\"] for seq in dataset[\"sequence\"]]\n",
    "labels = [label[:50] + [-100]*(50-len(label)) for label in dataset[\"labels\"]] # Truncate/pad labels to match input IDs\n",
    "\n",
    "\n",
    "# Create ProteinData objects and then ProteinDataset\n",
    "data = [ProteinData(input_id, label) for input_id, label in zip(inputs, labels)]\n",
    "protein_dataset = ProteinDataset(data)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,   # increase the number of epochs\n",
    "    per_device_train_batch_size=32,  # increase if you have enough memory\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=protein_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"trained_token_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 1 - M: Binding Site\n",
      "Position 2 - A: Binding Site\n",
      "Position 3 - P: Binding Site\n",
      "Position 4 - L: Binding Site\n",
      "Position 5 - R: Binding Site\n",
      "Position 6 - K: Binding Site\n",
      "Position 7 - T: Binding Site\n",
      "Position 8 - Y: Exposed to Solvent\n",
      "Position 9 - V: Exposed to Solvent\n",
      "Position 10 - L: Binding Site\n",
      "Position 11 - K: Exposed to Solvent\n",
      "Position 12 - L: Exposed to Solvent\n",
      "Position 13 - Y: Exposed to Solvent\n",
      "Position 14 - V: Exposed to Solvent\n",
      "Position 15 - A: Binding Site\n",
      "Position 16 - G: Binding Site\n",
      "Position 17 - N: Binding Site\n",
      "Position 18 - T: Exposed to Solvent\n",
      "Position 19 - P: Exposed to Solvent\n",
      "Position 20 - N: Exposed to Solvent\n",
      "Position 21 - S: Exposed to Solvent\n",
      "Position 22 - V: Exposed to Solvent\n",
      "Position 23 - R: Exposed to Solvent\n",
      "Position 24 - A: Exposed to Solvent\n",
      "Position 25 - L: Binding Site\n",
      "Position 26 - K: Exposed to Solvent\n",
      "Position 27 - T: Exposed to Solvent\n",
      "Position 28 - L: Exposed to Solvent\n",
      "Position 29 - N: Exposed to Solvent\n",
      "Position 30 - N: Exposed to Solvent\n",
      "Position 31 - I: Exposed to Solvent\n",
      "Position 32 - L: Binding Site\n",
      "Position 33 - E: Exposed to Solvent\n",
      "Position 34 - K: Exposed to Solvent\n",
      "Position 35 - E: Binding Site\n",
      "Position 36 - F: Exposed to Solvent\n",
      "Position 37 - K: Exposed to Solvent\n",
      "Position 38 - G: Binding Site\n",
      "Position 39 - V: Binding Site\n",
      "Position 40 - Y: Exposed to Solvent\n",
      "Position 41 - A: Exposed to Solvent\n",
      "Position 42 - L: Binding Site\n",
      "Position 43 - K: Exposed to Solvent\n",
      "Position 44 - V: Exposed to Solvent\n",
      "Position 45 - I: Exposed to Solvent\n",
      "Position 46 - D: Binding Site\n",
      "Position 47 - V: Binding Site\n",
      "Position 48 - L: Binding Site\n",
      "Position 49 - K: Exposed to Solvent\n",
      "Position 50 - N: Exposed to Solvent\n",
      "Position 51 - P: Exposed to Solvent\n",
      "Position 52 - Q: Exposed to Solvent\n",
      "Position 53 - L: Exposed to Solvent\n",
      "Position 54 - A: Exposed to Solvent\n",
      "Position 55 - E: Binding Site\n",
      "Position 56 - E: Exposed to Solvent\n",
      "Position 57 - D: Exposed to Solvent\n",
      "Position 58 - K: Binding Site\n",
      "Position 59 - I: Exposed to Solvent\n",
      "Position 60 - L: Exposed to Solvent\n",
      "Position 61 - A: Exposed to Solvent\n",
      "Position 62 - T: Binding Site\n",
      "Position 63 - P: Exposed to Solvent\n",
      "Position 64 - T: Exposed to Solvent\n",
      "Position 65 - L: Exposed to Solvent\n",
      "Position 66 - A: Exposed to Solvent\n",
      "Position 67 - K: Exposed to Solvent\n",
      "Position 68 - V: Exposed to Solvent\n",
      "Position 69 - L: Binding Site\n",
      "Position 70 - P: Exposed to Solvent\n",
      "Position 71 - P: Binding Site\n",
      "Position 72 - P: Binding Site\n",
      "Position 73 - V: Binding Site\n",
      "Position 74 - R: Binding Site\n",
      "Position 75 - R: Binding Site\n",
      "Position 76 - I: Binding Site\n",
      "Position 77 - I: Binding Site\n",
      "Position 78 - G: Binding Site\n",
      "Position 79 - D: Binding Site\n",
      "Position 80 - L: Binding Site\n",
      "Position 81 - S: Binding Site\n",
      "Position 82 - N: Exposed to Solvent\n",
      "Position 83 - R: Exposed to Solvent\n",
      "Position 84 - E: Binding Site\n",
      "Position 85 - K: Exposed to Solvent\n",
      "Position 86 - V: Binding Site\n",
      "Position 87 - L: Exposed to Solvent\n",
      "Position 88 - I: Exposed to Solvent\n",
      "Position 89 - G: Binding Site\n",
      "Position 90 - L: Binding Site\n",
      "Position 91 - D: Binding Site\n",
      "Position 92 - L: Binding Site\n",
      "Position 93 - L: Binding Site\n",
      "Position 94 - Y: Exposed to Solvent\n",
      "Position 95 - E: Exposed to Solvent\n",
      "Position 96 - E: Exposed to Solvent\n",
      "Position 97 - I: Exposed to Solvent\n",
      "Position 98 - G: Binding Site\n",
      "Position 99 - D: Binding Site\n",
      "Position 100 - Q: Binding Site\n",
      "Position 101 - A: Binding Site\n",
      "Position 102 - E: Binding Site\n",
      "Position 103 - D: Exposed to Solvent\n",
      "Position 104 - D: Binding Site\n",
      "Position 105 - L: Binding Site\n",
      "Position 106 - G: Binding Site\n",
      "Position 107 - L: Binding Site\n",
      "Position 108 - E: Binding Site\n"
     ]
    }
   ],
   "source": [
    "# Let's say this is your new sequence\n",
    "new_sequence = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"\n",
    "\n",
    "# Convert sequence to input IDs\n",
    "inputs = tokenizer(new_sequence, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Apply model to get the logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# Get the predictions by picking the label (class) with the highest logit\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "# Here we map each label number to its corresponding class\n",
    "class_mapping = {\n",
    "    0: 'Exposed to Solvent',\n",
    "    1: 'Binding Site',\n",
    "    2: 'Transmembrane region',\n",
    "}\n",
    "\n",
    "# Then, we convert the tensor to a list of integers\n",
    "prediction_list = predictions.tolist()[0]\n",
    "\n",
    "# Now, we get a list of class labels using the mapping\n",
    "class_labels = [class_mapping[pred] for pred in prediction_list]\n",
    "\n",
    "# Create a list that matches each amino acid in the sequence to its predicted class label\n",
    "residue_to_label = list(zip(list(new_sequence), class_labels))\n",
    "\n",
    "# Print out the list\n",
    "for i, (residue, label) in enumerate(residue_to_label):\n",
    "    print(f\"Position {i+1} - {residue}: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
